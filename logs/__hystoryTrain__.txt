Training loss on iteration 0 = 1.3875919580459595
Training loss on iteration 1 = 1.9157721996307373
Training loss on iteration 2 = 1.8055776357650757
Training loss on iteration 3 = 1.104295253753662
Training loss on iteration 4 = 1.178458333015442
Training loss on iteration 5 = 1.071778655052185
Training loss on iteration 6 = 1.4508789777755737
Training loss on iteration 7 = 1.1154695749282837
Training loss on iteration 8 = 1.0541889667510986
Training loss on iteration 9 = 1.0584077835083008
Training loss on iteration 10 = 0.8392971158027649
Training loss on iteration 11 = 1.1255545616149902
Training loss on iteration 12 = 1.0972676277160645
Training loss on iteration 13 = 0.927863597869873
Training loss on iteration 14 = 0.8175411224365234
Training loss on iteration 15 = 0.9341017007827759
Training loss on iteration 16 = 0.8285195231437683
Training loss on iteration 17 = 0.6826969981193542
Training loss on iteration 18 = 0.6708621382713318
Training loss on iteration 19 = 0.7197176218032837
Training loss on iteration 20 = 1.027290940284729
Training loss on iteration 21 = 0.5612491369247437
Training loss on iteration 22 = 1.364627718925476
Training loss on iteration 23 = 0.6753945350646973
Training loss on iteration 24 = 1.0003970861434937
Training loss on iteration 25 = 0.7715190649032593
Training loss on iteration 26 = 0.943687379360199
Training loss on iteration 27 = 0.8870744705200195
Training loss on iteration 28 = 0.679287850856781
Training loss on iteration 29 = 0.5708370208740234
Training loss on iteration 30 = 0.454720139503479
Training loss on iteration 31 = 0.5569199919700623
Training loss on iteration 32 = 0.684269905090332
Training loss on iteration 33 = 0.8206787705421448
Training loss on iteration 34 = 0.8789482712745667
Training loss on iteration 35 = 0.7373936772346497
Training loss on iteration 36 = 0.5353120565414429
Training loss on iteration 37 = 0.916235625743866
Training loss on iteration 38 = 0.6901072859764099
Training loss on iteration 39 = 0.6446912288665771
Training loss on iteration 40 = 0.8099984526634216
Training loss on iteration 41 = 0.7702153921127319
Training loss on iteration 42 = 0.832466721534729
Training loss on iteration 43 = 0.652563214302063
Training loss on iteration 44 = 0.6368412375450134
Training loss on iteration 45 = 0.3780372738838196
Training loss on iteration 46 = 0.8075178265571594
Training loss on iteration 47 = 0.5451932549476624
Training loss on iteration 48 = 0.5525875687599182
Training loss on iteration 49 = 0.4290578365325928
Training loss on iteration 50 = 0.6931850910186768
Training loss on iteration 51 = 0.6194054484367371
Training loss on iteration 52 = 0.8693698644638062
Training loss on iteration 53 = 0.6555953025817871
Training loss on iteration 54 = 0.7469016909599304
Training loss on iteration 55 = 0.6147657632827759
Training loss on iteration 56 = 0.5958592295646667
Training loss on iteration 57 = 0.8823708891868591
Training loss on iteration 58 = 0.5984449982643127
Training loss on iteration 59 = 0.8169283866882324
Training loss on iteration 60 = 0.38708701729774475
Training loss on iteration 61 = 0.3546372354030609
Training loss on iteration 62 = 0.456971675157547
Training loss on iteration 63 = 0.657358705997467
Training loss on iteration 64 = 0.661268413066864
Training loss on iteration 65 = 0.4546957015991211
Training loss on iteration 66 = 0.7098647952079773
Training loss on iteration 67 = 0.6658012866973877
Training loss on iteration 68 = 0.48218202590942383
Training loss on iteration 69 = 0.5341090559959412
Training loss on iteration 70 = 0.6177056431770325
Training loss on iteration 71 = 0.49010249972343445
Training loss on iteration 72 = 0.6129507422447205
Training loss on iteration 73 = 0.8263860940933228
Training loss on iteration 74 = 0.42640385031700134
Training loss on iteration 75 = 0.6086141467094421
Training loss on iteration 76 = 0.6382410526275635
Training loss on iteration 77 = 0.24282744526863098
Training loss on iteration 78 = 0.601769745349884
Training loss on iteration 79 = 0.6659510731697083
Training loss on iteration 80 = 0.3709423243999481
Training loss on iteration 81 = 0.47186359763145447
Training loss on iteration 82 = 0.73797208070755
Training loss on iteration 83 = 0.9239094257354736
Training loss on iteration 84 = 0.2550017237663269
Training loss on iteration 85 = 0.4645725190639496
Training loss on iteration 86 = 0.3404593765735626
Training loss on iteration 87 = 0.6355834007263184
Training loss on iteration 88 = 0.5892293453216553
Training loss on iteration 89 = 0.20795297622680664
Training loss on iteration 90 = 0.47835907340049744
Training loss on iteration 91 = 0.24712419509887695
Training loss on iteration 92 = 0.5908310413360596
Training loss on iteration 93 = 0.3698878288269043
Training loss on iteration 94 = 0.6912771463394165
Training loss on iteration 95 = 0.6746042966842651
Training loss on iteration 96 = 0.7560959458351135
Training loss on iteration 97 = 0.5324956774711609
Training loss on iteration 98 = 0.3529264032840729
Training loss on iteration 99 = 0.6177479028701782
Training loss on iteration 100 = 0.7370968461036682
Training loss on iteration 101 = 0.28179439902305603
Training loss on iteration 102 = 0.4413521885871887
Training loss on iteration 103 = 0.6864079236984253
Training loss on iteration 104 = 0.5059490203857422
Training loss on iteration 105 = 0.41139474511146545
Training loss on iteration 106 = 0.39299309253692627
Training loss on iteration 107 = 0.3549686670303345
Training loss on iteration 108 = 0.300895094871521
Training loss on iteration 109 = 0.3941417932510376
Training loss on iteration 110 = 0.4324890375137329
Training loss on iteration 111 = 0.6280128955841064
Training loss on iteration 112 = 0.3540584146976471
Training loss on iteration 113 = 0.40775415301322937
Training loss on iteration 114 = 0.4967487156391144
Training loss on iteration 115 = 0.5435750484466553
Training loss on iteration 116 = 0.5935533046722412
Training loss on iteration 117 = 0.5015402436256409
Training loss on iteration 118 = 0.5064414739608765
Training loss on iteration 119 = 0.4163282513618469
Training loss on iteration 120 = 0.6625692248344421
Training loss on iteration 121 = 0.47230109572410583
Training loss on iteration 122 = 0.5178651809692383
Training loss on iteration 123 = 0.417702853679657
Training loss on iteration 124 = 0.32163429260253906
Training loss on iteration 0 = 0.4180002808570862
Training loss on iteration 1 = 0.3165982961654663
Training loss on iteration 2 = 0.4006800949573517
Training loss on iteration 3 = 0.6019718050956726
Training loss on iteration 4 = 0.46401655673980713
Training loss on iteration 5 = 0.8190100789070129
Training loss on iteration 6 = 0.6289857625961304
Training loss on iteration 7 = 0.5518589019775391
Training loss on iteration 8 = 0.4157196879386902
Training loss on iteration 9 = 0.2225208580493927
Training loss on iteration 10 = 0.658457338809967
Training loss on iteration 11 = 0.7758631706237793
Training loss on iteration 12 = 0.21671263873577118
Training loss on iteration 13 = 0.4527648985385895
Training loss on iteration 14 = 0.4639434814453125
Training loss on iteration 15 = 0.395965576171875
Training loss on iteration 16 = 0.2761516273021698
Training loss on iteration 17 = 0.3449833393096924
Training loss on iteration 18 = 0.9633699655532837
Training loss on iteration 19 = 0.42975157499313354
Training loss on iteration 20 = 0.3336608409881592
Training loss on iteration 21 = 0.4105597138404846
Training loss on iteration 22 = 0.23303000628948212
Training loss on iteration 23 = 0.3704253137111664
Training loss on iteration 24 = 0.5474373698234558
Training loss on iteration 25 = 0.3918675482273102
Training loss on iteration 26 = 0.36001408100128174
Training loss on iteration 27 = 0.3537691831588745
Training loss on iteration 28 = 0.36689192056655884
Training loss on iteration 29 = 0.29849323630332947
Training loss on iteration 30 = 0.37487128376960754
Training loss on iteration 31 = 0.2904556095600128
Training loss on iteration 32 = 0.2542426288127899
Training loss on iteration 33 = 0.279261976480484
Training loss on iteration 34 = 0.4364001750946045
Training loss on iteration 35 = 0.5250160098075867
Training loss on iteration 36 = 0.6518465280532837
Training loss on iteration 37 = 0.3425081968307495
Training loss on iteration 38 = 0.21688923239707947
Training loss on iteration 39 = 0.36476776003837585
Training loss on iteration 40 = 0.38228896260261536
Training loss on iteration 41 = 0.19704559445381165
Training loss on iteration 42 = 0.3294408619403839
Training loss on iteration 43 = 0.2295493483543396
Training loss on iteration 44 = 0.3650961220264435
Training loss on iteration 45 = 0.387169748544693
Training loss on iteration 46 = 0.2692983150482178
Training loss on iteration 47 = 0.3983806371688843
Training loss on iteration 48 = 0.49760761857032776
Training loss on iteration 49 = 0.41172435879707336
Training loss on iteration 50 = 0.4580756723880768
Training loss on iteration 51 = 0.3762051463127136
Training loss on iteration 52 = 0.19427379965782166
Training loss on iteration 53 = 0.27335476875305176
Training loss on iteration 54 = 0.3376140594482422
Training loss on iteration 55 = 0.19232900440692902
Training loss on iteration 56 = 0.34339356422424316
Training loss on iteration 57 = 0.3959384858608246
Training loss on iteration 58 = 0.31480303406715393
Training loss on iteration 59 = 0.6423092484474182
Training loss on iteration 60 = 0.2729223966598511
Training loss on iteration 61 = 0.7039781212806702
Training loss on iteration 62 = 0.23121555149555206
Training loss on iteration 63 = 0.6491535305976868
Training loss on iteration 64 = 0.8633524179458618
Training loss on iteration 65 = 0.27260056138038635
Training loss on iteration 66 = 0.5118030309677124
Training loss on iteration 67 = 0.27413472533226013
Training loss on iteration 68 = 0.35299253463745117
Training loss on iteration 69 = 0.4110490679740906
Training loss on iteration 70 = 0.6905196905136108
Training loss on iteration 71 = 0.6221967339515686
Training loss on iteration 72 = 0.3722919821739197
Training loss on iteration 73 = 0.4142087697982788
Training loss on iteration 74 = 0.5787692666053772
Training loss on iteration 75 = 0.7241202592849731
Training loss on iteration 76 = 0.3889079689979553
Training loss on iteration 77 = 0.5438283681869507
Training loss on iteration 78 = 0.35162436962127686
Training loss on iteration 79 = 0.436583012342453
Training loss on iteration 80 = 0.3758433759212494
Training loss on iteration 81 = 0.2983333170413971
Training loss on iteration 82 = 0.2884823977947235
Training loss on iteration 83 = 0.2879512310028076
Training loss on iteration 84 = 0.47284647822380066
Training loss on iteration 85 = 0.6063024401664734
Training loss on iteration 86 = 0.41082754731178284
Training loss on iteration 87 = 0.28958743810653687
Training loss on iteration 88 = 0.3454545736312866
Training loss on iteration 89 = 0.28753164410591125
Training loss on iteration 90 = 0.2827042043209076
Training loss on iteration 91 = 0.36871346831321716
Training loss on iteration 92 = 0.4806751310825348
Training loss on iteration 93 = 0.16544818878173828
Training loss on iteration 94 = 0.3533090353012085
Training loss on iteration 95 = 0.37697863578796387
Training loss on iteration 96 = 0.29205557703971863
Training loss on iteration 97 = 0.5374577045440674
Training loss on iteration 98 = 0.3478030264377594
Training loss on iteration 99 = 0.6055613160133362
Training loss on iteration 100 = 0.4956527054309845
Training loss on iteration 101 = 0.24557264149188995
Training loss on iteration 102 = 0.29978296160697937
Training loss on iteration 103 = 0.2280188500881195
Training loss on iteration 104 = 0.42974138259887695
Training loss on iteration 105 = 0.15559808909893036
Training loss on iteration 106 = 0.3516533672809601
Training loss on iteration 107 = 0.39277881383895874
Training loss on iteration 108 = 0.48967263102531433
Training loss on iteration 109 = 0.434146523475647
Training loss on iteration 110 = 0.3725358247756958
Training loss on iteration 111 = 0.5135748386383057
Training loss on iteration 112 = 0.38066205382347107
Training loss on iteration 113 = 0.39752522110939026
Training loss on iteration 114 = 0.23111698031425476
Training loss on iteration 115 = 0.23480607569217682
Training loss on iteration 116 = 0.813887357711792
Training loss on iteration 117 = 0.18266688287258148
Training loss on iteration 118 = 0.22710180282592773
Training loss on iteration 119 = 0.2900489568710327
Training loss on iteration 120 = 0.197422593832016
Training loss on iteration 121 = 0.2960517406463623
Training loss on iteration 122 = 0.18785716593265533
Training loss on iteration 123 = 0.32757169008255005
Training loss on iteration 124 = 0.3336997926235199
Training loss on iteration 0 = 0.41056928038597107
Training loss on iteration 1 = 0.14325103163719177
Training loss on iteration 2 = 0.18542981147766113
Training loss on iteration 3 = 0.24583181738853455
Training loss on iteration 4 = 0.3538122773170471
Training loss on iteration 5 = 0.2358102798461914
Training loss on iteration 6 = 0.2321978658437729
Training loss on iteration 7 = 0.22401846945285797
Training loss on iteration 8 = 0.26775071024894714
Training loss on iteration 9 = 0.4621252119541168
Training loss on iteration 10 = 0.4169304072856903
Training loss on iteration 11 = 0.2249409407377243
Training loss on iteration 12 = 0.3716112971305847
Training loss on iteration 13 = 0.2423236221075058
Training loss on iteration 14 = 0.5356200933456421
Training loss on iteration 15 = 0.54698646068573
Training loss on iteration 16 = 0.6043558120727539
Training loss on iteration 17 = 0.6836014986038208
Training loss on iteration 18 = 0.551803708076477
Training loss on iteration 19 = 0.391446590423584
Training loss on iteration 20 = 0.21373222768306732
Training loss on iteration 21 = 0.24386292695999146
Training loss on iteration 22 = 0.4335867166519165
Training loss on iteration 23 = 0.20914021134376526
Training loss on iteration 24 = 0.24083924293518066
Training loss on iteration 25 = 0.34169092774391174
Training loss on iteration 26 = 0.4395294785499573
Training loss on iteration 27 = 0.28823891282081604
Training loss on iteration 28 = 0.19709539413452148
Training loss on iteration 29 = 0.20120389759540558
Training loss on iteration 30 = 0.2066439390182495
Training loss on iteration 31 = 0.14050237834453583
Training loss on iteration 32 = 0.17299790680408478
Training loss on iteration 33 = 0.1820969134569168
Training loss on iteration 34 = 0.3673589527606964
Training loss on iteration 35 = 0.40593433380126953
Training loss on iteration 36 = 0.4545386731624603
Training loss on iteration 37 = 0.5060135126113892
Training loss on iteration 38 = 0.45394712686538696
Training loss on iteration 39 = 0.20591993629932404
Training loss on iteration 40 = 0.17894534766674042
Training loss on iteration 41 = 0.34594449400901794
Training loss on iteration 42 = 0.20126649737358093
Training loss on iteration 43 = 0.26289787888526917
Training loss on iteration 44 = 0.4271486699581146
Training loss on iteration 45 = 0.2559950053691864
Training loss on iteration 46 = 0.21731112897396088
Training loss on iteration 47 = 0.25208500027656555
Training loss on iteration 48 = 0.2528149485588074
Training loss on iteration 49 = 0.22441217303276062
Training loss on iteration 50 = 0.2907603979110718
Training loss on iteration 51 = 0.3144446611404419
Training loss on iteration 52 = 0.43785950541496277
Training loss on iteration 53 = 0.14160339534282684
Training loss on iteration 54 = 0.4013363718986511
Training loss on iteration 55 = 0.33260107040405273
Training loss on iteration 56 = 0.3305686414241791
Training loss on iteration 57 = 0.4806469976902008
Training loss on iteration 58 = 0.4028759300708771
Training loss on iteration 59 = 0.3639504015445709
Training loss on iteration 60 = 0.15633144974708557
Training loss on iteration 61 = 0.38103535771369934
Training loss on iteration 62 = 0.2921026349067688
Training loss on iteration 63 = 0.644025444984436
Training loss on iteration 64 = 0.32310208678245544
Training loss on iteration 65 = 0.6561378836631775
Training loss on iteration 66 = 0.1280316710472107
Training loss on iteration 67 = 0.27964675426483154
Training loss on iteration 68 = 0.24939261376857758
Training loss on iteration 69 = 0.2588712275028229
Training loss on iteration 70 = 0.3574594259262085
Training loss on iteration 71 = 0.39825111627578735
Training loss on iteration 72 = 0.22314302623271942
Training loss on iteration 73 = 0.2698916494846344
Training loss on iteration 74 = 0.2243226319551468
Training loss on iteration 75 = 0.15013450384140015
Training loss on iteration 76 = 0.2621682584285736
Training loss on iteration 77 = 0.22463837265968323
Training loss on iteration 78 = 0.2574690282344818
Training loss on iteration 79 = 0.26370176672935486
Training loss on iteration 80 = 0.23612777888774872
Training loss on iteration 81 = 0.4690414071083069
Training loss on iteration 82 = 0.409273624420166
Training loss on iteration 83 = 0.21529489755630493
Training loss on iteration 84 = 0.1611839234828949
Training loss on iteration 85 = 0.3679071068763733
Training loss on iteration 86 = 0.2661850154399872
Training loss on iteration 87 = 0.34343841671943665
Training loss on iteration 88 = 0.2695434093475342
Training loss on iteration 89 = 0.6199330687522888
Training loss on iteration 90 = 0.40564125776290894
Training loss on iteration 91 = 0.35017380118370056
Training loss on iteration 92 = 0.19331924617290497
Training loss on iteration 93 = 0.3661423921585083
Training loss on iteration 94 = 0.3377634286880493
Training loss on iteration 95 = 0.3330540359020233
Training loss on iteration 96 = 0.3244773745536804
Training loss on iteration 97 = 0.28316730260849
Training loss on iteration 98 = 0.4307679235935211
Training loss on iteration 99 = 0.19098606705665588
Training loss on iteration 100 = 0.27441301941871643
Training loss on iteration 101 = 0.3255268633365631
Training loss on iteration 102 = 0.34405460953712463
Training loss on iteration 103 = 0.26777228713035583
Training loss on iteration 104 = 0.34028229117393494
Training loss on iteration 105 = 0.24949923157691956
Training loss on iteration 106 = 0.16290390491485596
Training loss on iteration 107 = 0.40309715270996094
Training loss on iteration 108 = 0.24442631006240845
Training loss on iteration 109 = 0.18791106343269348
Training loss on iteration 110 = 0.3709033131599426
Training loss on iteration 111 = 0.4394984245300293
Training loss on iteration 112 = 0.23981115221977234
Training loss on iteration 113 = 0.3019463121891022
Training loss on iteration 114 = 0.28902217745780945
Training loss on iteration 115 = 0.2526143193244934
Training loss on iteration 116 = 0.22911372780799866
Training loss on iteration 117 = 0.24952208995819092
Training loss on iteration 118 = 0.21422863006591797
Training loss on iteration 119 = 0.3026745915412903
Training loss on iteration 120 = 0.34795090556144714
Training loss on iteration 121 = 0.1632598340511322
Training loss on iteration 122 = 0.45455533266067505
Training loss on iteration 123 = 0.35490694642066956
Training loss on iteration 124 = 0.14532393217086792
Training loss on iteration 0 = 0.3947252035140991
Training loss on iteration 1 = 0.3543851673603058
Training loss on iteration 2 = 0.2087569236755371
Training loss on iteration 3 = 0.16043943166732788
Training loss on iteration 4 = 0.3025381863117218
Training loss on iteration 5 = 0.16511791944503784
Training loss on iteration 6 = 0.33342188596725464
Training loss on iteration 7 = 0.35506415367126465
Training loss on iteration 8 = 0.16953781247138977
Training loss on iteration 9 = 0.5847606062889099
Training loss on iteration 10 = 0.41321080923080444
Training loss on iteration 11 = 0.3141240179538727
Training loss on iteration 12 = 0.2808310091495514
Training loss on iteration 13 = 0.1563878208398819
Training loss on iteration 14 = 0.15292777121067047
Training loss on iteration 15 = 0.13796822726726532
Training loss on iteration 16 = 0.2888651490211487
Training loss on iteration 17 = 0.14978395402431488
Training loss on iteration 18 = 0.525592565536499
Training loss on iteration 19 = 0.23804102838039398
Training loss on iteration 20 = 0.33143511414527893
Training loss on iteration 21 = 0.2332264482975006
Training loss on iteration 22 = 0.1181454211473465
Training loss on iteration 23 = 0.15339383482933044
Training loss on iteration 24 = 0.14333884418010712
Training loss on iteration 25 = 0.18406742811203003
Training loss on iteration 26 = 0.2645285427570343
Training loss on iteration 27 = 0.36015433073043823
Training loss on iteration 28 = 0.47977009415626526
Training loss on iteration 29 = 0.16104833781719208
Training loss on iteration 30 = 0.23363259434700012
Training loss on iteration 31 = 0.2080952674150467
Training loss on iteration 32 = 0.46821507811546326
Training loss on iteration 33 = 0.4432668387889862
Training loss on iteration 34 = 0.16189394891262054
Training loss on iteration 35 = 0.2927982807159424
Training loss on iteration 36 = 0.15913774073123932
Training loss on iteration 37 = 0.20771342515945435
Training loss on iteration 38 = 0.3914937973022461
Training loss on iteration 39 = 0.18827663362026215
Training loss on iteration 40 = 0.20401206612586975
Training loss on iteration 41 = 0.15759755671024323
Training loss on iteration 42 = 0.13305576145648956
Training loss on iteration 43 = 0.1586991399526596
Training loss on iteration 44 = 0.22327561676502228
Training loss on iteration 45 = 0.07255309820175171
Training loss on iteration 46 = 0.3421177268028259
Training loss on iteration 47 = 0.2938738167285919
Training loss on iteration 48 = 0.2665029466152191
Training loss on iteration 49 = 0.17806296050548553
Training loss on iteration 50 = 0.1850121170282364
Training loss on iteration 51 = 0.4307292103767395
Training loss on iteration 52 = 0.23890651762485504
Training loss on iteration 53 = 0.17456576228141785
Training loss on iteration 54 = 0.06959813833236694
Training loss on iteration 55 = 0.10437590628862381
Training loss on iteration 56 = 0.25068560242652893
Training loss on iteration 57 = 0.16850535571575165
Training loss on iteration 58 = 0.21952536702156067
Training loss on iteration 59 = 0.28352418541908264
Training loss on iteration 60 = 0.16773371398448944
Training loss on iteration 61 = 0.1325206309556961
Training loss on iteration 62 = 0.23222166299819946
Training loss on iteration 63 = 0.25455236434936523
Training loss on iteration 64 = 0.3772055208683014
Training loss on iteration 65 = 0.19438156485557556
Training loss on iteration 66 = 0.2434621900320053
Training loss on iteration 67 = 0.3500204384326935
Training loss on iteration 68 = 0.1294977217912674
Training loss on iteration 69 = 0.1618465930223465
Training loss on iteration 70 = 0.17630594968795776
Training loss on iteration 71 = 0.13160572946071625
Training loss on iteration 72 = 0.13385745882987976
Training loss on iteration 73 = 0.050059884786605835
Training loss on iteration 74 = 0.09742085635662079
Training loss on iteration 75 = 0.1883523017168045
Training loss on iteration 76 = 0.16920636594295502
Training loss on iteration 77 = 0.20115405321121216
Training loss on iteration 78 = 0.15122129023075104
Training loss on iteration 79 = 0.1568227857351303
Training loss on iteration 80 = 0.5018322467803955
Training loss on iteration 81 = 0.15786053240299225
Training loss on iteration 82 = 0.15794675052165985
Training loss on iteration 83 = 0.1732906997203827
Training loss on iteration 84 = 0.3665499687194824
Training loss on iteration 85 = 0.1620619297027588
Training loss on iteration 86 = 0.12252403795719147
Training loss on iteration 87 = 0.3717668950557709
Training loss on iteration 88 = 0.10689228028059006
Training loss on iteration 89 = 0.466559499502182
Training loss on iteration 90 = 0.4199102222919464
Training loss on iteration 91 = 0.17339852452278137
Training loss on iteration 92 = 0.2546024024486542
Training loss on iteration 93 = 0.3088454306125641
Training loss on iteration 94 = 0.3047737777233124
Training loss on iteration 95 = 0.2636198401451111
Training loss on iteration 96 = 0.1297490894794464
Training loss on iteration 97 = 0.1045159101486206
Training loss on iteration 98 = 0.28462836146354675
Training loss on iteration 99 = 0.448144793510437
Training loss on iteration 100 = 0.16564874351024628
Training loss on iteration 101 = 0.24362865090370178
Training loss on iteration 102 = 0.15870164334774017
Training loss on iteration 103 = 0.3109346032142639
Training loss on iteration 104 = 0.30666136741638184
Training loss on iteration 105 = 0.20057563483715057
Training loss on iteration 106 = 0.2635971009731293
Training loss on iteration 107 = 0.1960732638835907
Training loss on iteration 108 = 0.11237399280071259
Training loss on iteration 109 = 0.14973028004169464
Training loss on iteration 110 = 0.2290753275156021
Training loss on iteration 111 = 0.3202891945838928
Training loss on iteration 112 = 0.26304373145103455
Training loss on iteration 113 = 0.16190947592258453
Training loss on iteration 114 = 0.25011512637138367
Training loss on iteration 115 = 0.17443694174289703
Training loss on iteration 116 = 0.42188164591789246
Training loss on iteration 117 = 0.09164227545261383
Training loss on iteration 118 = 0.18558840453624725
Training loss on iteration 119 = 0.35446813702583313
Training loss on iteration 120 = 0.417919784784317
Training loss on iteration 121 = 0.1998683363199234
Training loss on iteration 122 = 0.30546846985816956
Training loss on iteration 123 = 0.2991345524787903
Training loss on iteration 124 = 0.031225813552737236
Training loss on iteration 0 = 0.4032912254333496
Training loss on iteration 1 = 0.29630646109580994
Training loss on iteration 2 = 0.2745266556739807
Training loss on iteration 3 = 0.16986477375030518
Training loss on iteration 4 = 0.38087037205696106
Training loss on iteration 5 = 0.3774174749851227
Training loss on iteration 6 = 0.2075669765472412
Training loss on iteration 7 = 0.17147043347358704
Training loss on iteration 8 = 0.17037387192249298
Training loss on iteration 9 = 0.2159552276134491
Training loss on iteration 10 = 0.2595781087875366
Training loss on iteration 11 = 0.10495686531066895
Training loss on iteration 12 = 0.2114085704088211
Training loss on iteration 13 = 0.31091099977493286
Training loss on iteration 14 = 0.26348161697387695
Training loss on iteration 15 = 0.22091034054756165
Training loss on iteration 16 = 0.24025964736938477
Training loss on iteration 17 = 0.15374743938446045
Training loss on iteration 18 = 0.35376307368278503
Training loss on iteration 19 = 0.2229883074760437
Training loss on iteration 20 = 0.1467217057943344
Training loss on iteration 21 = 0.19744578003883362
Training loss on iteration 22 = 0.2587096393108368
Training loss on iteration 23 = 0.2066962867975235
Training loss on iteration 24 = 0.19441457092761993
Training loss on iteration 25 = 0.12397117167711258
Training loss on iteration 26 = 0.24734865128993988
Training loss on iteration 27 = 0.1670205444097519
Training loss on iteration 28 = 0.12989197671413422
Training loss on iteration 29 = 0.31399932503700256
Training loss on iteration 30 = 0.16549071669578552
Training loss on iteration 31 = 0.25026676058769226
Training loss on iteration 32 = 0.06578605622053146
Training loss on iteration 33 = 0.1306590586900711
Training loss on iteration 34 = 0.28634288907051086
Training loss on iteration 35 = 0.30047789216041565
Training loss on iteration 36 = 0.18899890780448914
Training loss on iteration 37 = 0.2178005576133728
Training loss on iteration 38 = 0.1034185066819191
Training loss on iteration 39 = 0.3313792943954468
Training loss on iteration 40 = 0.20197096467018127
Training loss on iteration 41 = 0.1498543918132782
Training loss on iteration 42 = 0.39674296975135803
Training loss on iteration 43 = 0.25199612975120544
Training loss on iteration 44 = 0.23860898613929749
Training loss on iteration 45 = 0.16067568957805634
Training loss on iteration 46 = 0.09157904237508774
Training loss on iteration 47 = 0.4706067740917206
Training loss on iteration 48 = 0.2554713487625122
Training loss on iteration 49 = 0.0778147354722023
Training loss on iteration 50 = 0.21219022572040558
Training loss on iteration 51 = 0.38736388087272644
Training loss on iteration 52 = 0.1594412922859192
Training loss on iteration 53 = 0.36832597851753235
Training loss on iteration 54 = 0.1656523048877716
Training loss on iteration 55 = 0.2545793652534485
Training loss on iteration 56 = 0.44467106461524963
Training loss on iteration 57 = 0.19201625883579254
Training loss on iteration 58 = 0.3646480143070221
Training loss on iteration 59 = 0.1756337583065033
Training loss on iteration 60 = 0.09685876220464706
Training loss on iteration 61 = 0.3269825577735901
Training loss on iteration 62 = 0.33082297444343567
Training loss on iteration 63 = 0.09973826259374619
Training loss on iteration 64 = 0.23125788569450378
Training loss on iteration 65 = 0.2629989683628082
Training loss on iteration 66 = 0.17994974553585052
Training loss on iteration 67 = 0.23983216285705566
Training loss on iteration 68 = 0.3413885831832886
Training loss on iteration 69 = 0.19384604692459106
Training loss on iteration 70 = 0.13525564968585968
Training loss on iteration 71 = 0.1772361844778061
Training loss on iteration 72 = 0.12237662076950073
Training loss on iteration 73 = 0.1256137639284134
Training loss on iteration 74 = 0.24107225239276886
Training loss on iteration 75 = 0.27084529399871826
Training loss on iteration 76 = 0.37193563580513
Training loss on iteration 77 = 0.28569892048835754
Training loss on iteration 78 = 0.24933123588562012
Training loss on iteration 79 = 0.32374146580696106
Training loss on iteration 80 = 0.19199450314044952
Training loss on iteration 81 = 0.07122504711151123
Training loss on iteration 82 = 0.1367107331752777
Training loss on iteration 83 = 0.12850821018218994
Training loss on iteration 84 = 0.09167804569005966
Training loss on iteration 85 = 0.06444799154996872
Training loss on iteration 86 = 0.07093554735183716
Training loss on iteration 87 = 0.12856648862361908
Training loss on iteration 88 = 0.19104619324207306
Training loss on iteration 89 = 0.1821107715368271
Training loss on iteration 90 = 0.34574180841445923
Training loss on iteration 91 = 0.08494260162115097
Training loss on iteration 92 = 0.18341417610645294
Training loss on iteration 93 = 0.1331835687160492
Training loss on iteration 94 = 0.18147704005241394
Training loss on iteration 95 = 0.0968206450343132
Training loss on iteration 96 = 0.1680934727191925
Training loss on iteration 97 = 0.07982097566127777
Training loss on iteration 98 = 0.0604720413684845
Training loss on iteration 99 = 0.4303172528743744
Training loss on iteration 100 = 0.08939410001039505
Training loss on iteration 101 = 0.4363574683666229
Training loss on iteration 102 = 0.10586107522249222
Training loss on iteration 103 = 0.13999225199222565
Training loss on iteration 104 = 0.19671005010604858
Training loss on iteration 105 = 0.22064287960529327
Training loss on iteration 106 = 0.18276721239089966
Training loss on iteration 107 = 0.20781321823596954
Training loss on iteration 108 = 0.1528140753507614
Training loss on iteration 109 = 0.17989838123321533
Training loss on iteration 110 = 0.1142956018447876
Training loss on iteration 111 = 0.22174271941184998
Training loss on iteration 112 = 0.12640181183815002
Training loss on iteration 113 = 0.11983252316713333
Training loss on iteration 114 = 0.15274828672409058
Training loss on iteration 115 = 0.19715259969234467
Training loss on iteration 116 = 0.09389805048704147
Training loss on iteration 117 = 0.4678330421447754
Training loss on iteration 118 = 0.3954317569732666
Training loss on iteration 119 = 0.5365265607833862
Training loss on iteration 120 = 0.29361236095428467
Training loss on iteration 121 = 0.16844286024570465
Training loss on iteration 122 = 0.15022437274456024
Training loss on iteration 123 = 0.14458845555782318
Training loss on iteration 124 = 0.22017411887645721
Training loss on iteration 0 = 0.14368562400341034
Training loss on iteration 1 = 0.1876356452703476
Training loss on iteration 2 = 0.21261687576770782
Training loss on iteration 3 = 0.06613019853830338
Training loss on iteration 4 = 0.27705317735671997
Training loss on iteration 5 = 0.042409494519233704
Training loss on iteration 6 = 0.10300137847661972
Training loss on iteration 7 = 0.21061500906944275
Training loss on iteration 8 = 0.16188347339630127
Training loss on iteration 9 = 0.15443076193332672
Training loss on iteration 10 = 0.544852077960968
Training loss on iteration 11 = 0.30285826325416565
Training loss on iteration 12 = 0.5490952134132385
Training loss on iteration 13 = 0.2965420186519623
Training loss on iteration 14 = 0.1436036080121994
Training loss on iteration 15 = 0.03906960040330887
Training loss on iteration 16 = 0.10289256274700165
Training loss on iteration 17 = 0.09086383879184723
Training loss on iteration 18 = 0.08906590193510056
Training loss on iteration 19 = 0.08597774803638458
Training loss on iteration 20 = 0.27906209230422974
Training loss on iteration 21 = 0.08091728389263153
Training loss on iteration 22 = 0.14590689539909363
Training loss on iteration 23 = 0.22087028622627258
Training loss on iteration 24 = 0.16377989947795868
Training loss on iteration 25 = 0.1981295496225357
Training loss on iteration 26 = 0.16240406036376953
Training loss on iteration 27 = 0.18607673048973083
Training loss on iteration 28 = 0.30575770139694214
Training loss on iteration 29 = 0.15992453694343567
Training loss on iteration 30 = 0.15221361815929413
Training loss on iteration 31 = 0.13373565673828125
Training loss on iteration 32 = 0.08590695261955261
Training loss on iteration 33 = 0.20813585817813873
Training loss on iteration 34 = 0.09797783195972443
Training loss on iteration 35 = 0.36246728897094727
Training loss on iteration 36 = 0.021232135593891144
Training loss on iteration 37 = 0.07378407567739487
Training loss on iteration 38 = 0.12163683772087097
Training loss on iteration 39 = 0.14221414923667908
Training loss on iteration 40 = 0.1858976036310196
Training loss on iteration 41 = 0.24377426505088806
Training loss on iteration 42 = 0.2014959305524826
Training loss on iteration 43 = 0.11932525038719177
Training loss on iteration 44 = 0.07602285593748093
Training loss on iteration 45 = 0.2787195146083832
Training loss on iteration 46 = 0.14958198368549347
Training loss on iteration 47 = 0.41727614402770996
Training loss on iteration 48 = 0.23277120292186737
Training loss on iteration 49 = 0.2655640244483948
Training loss on iteration 50 = 0.22417061030864716
Training loss on iteration 51 = 0.04651150107383728
Training loss on iteration 52 = 0.32241928577423096
Training loss on iteration 53 = 0.1294996738433838
Training loss on iteration 54 = 0.04971318691968918
Training loss on iteration 55 = 0.08895356208086014
Training loss on iteration 56 = 0.17695613205432892
Training loss on iteration 57 = 0.08836794644594193
Training loss on iteration 58 = 0.11976107209920883
Training loss on iteration 59 = 0.34914979338645935
Training loss on iteration 60 = 0.09022906422615051
Training loss on iteration 61 = 0.08150587230920792
Training loss on iteration 62 = 0.08314599096775055
Training loss on iteration 63 = 0.10286486148834229
Training loss on iteration 64 = 0.11207318305969238
Training loss on iteration 65 = 0.14928492903709412
Training loss on iteration 66 = 0.0843302309513092
Training loss on iteration 67 = 0.0692782998085022
Training loss on iteration 68 = 0.2278691530227661
Training loss on iteration 69 = 0.23666571080684662
Training loss on iteration 70 = 0.04281638562679291
Training loss on iteration 71 = 0.17725440859794617
Training loss on iteration 72 = 0.1795450896024704
Training loss on iteration 73 = 0.2768727242946625
Training loss on iteration 74 = 0.14014004170894623
Training loss on iteration 75 = 0.2337339222431183
Training loss on iteration 76 = 0.07999129593372345
Training loss on iteration 77 = 0.12253201752901077
Training loss on iteration 78 = 0.3844113349914551
Training loss on iteration 79 = 0.11132390797138214
Training loss on iteration 80 = 0.26499974727630615
Training loss on iteration 81 = 0.08258379250764847
Training loss on iteration 82 = 0.06731927394866943
Training loss on iteration 83 = 0.19135406613349915
Training loss on iteration 84 = 0.2880336046218872
Training loss on iteration 85 = 0.3032751977443695
Training loss on iteration 86 = 0.14581258594989777
Training loss on iteration 87 = 0.14405061304569244
Training loss on iteration 88 = 0.16449744999408722
Training loss on iteration 89 = 0.06906010210514069
Training loss on iteration 90 = 0.0960090309381485
Training loss on iteration 91 = 0.11050429940223694
Training loss on iteration 92 = 0.2654631435871124
Training loss on iteration 93 = 0.2234090268611908
Training loss on iteration 94 = 0.3247966170310974
Training loss on iteration 95 = 0.20454184710979462
Training loss on iteration 96 = 0.25782743096351624
Training loss on iteration 97 = 0.0931686982512474
Training loss on iteration 98 = 0.3069937229156494
Training loss on iteration 99 = 0.1556849628686905
Training loss on iteration 100 = 0.06397441774606705
Training loss on iteration 101 = 0.2684125602245331
Training loss on iteration 102 = 0.11319570988416672
Training loss on iteration 103 = 0.06926749646663666
Training loss on iteration 104 = 0.04264700412750244
Training loss on iteration 105 = 0.040609098970890045
Training loss on iteration 106 = 0.3569227159023285
Training loss on iteration 107 = 0.2988758385181427
Training loss on iteration 108 = 0.23789004981517792
Training loss on iteration 109 = 0.12414705753326416
Training loss on iteration 110 = 0.2704554498195648
Training loss on iteration 111 = 0.12622392177581787
Training loss on iteration 112 = 0.04085283353924751
Training loss on iteration 113 = 0.1749873161315918
Training loss on iteration 114 = 0.08559446781873703
Training loss on iteration 115 = 0.26662468910217285
Training loss on iteration 116 = 0.16817176342010498
Training loss on iteration 117 = 0.140268012881279
Training loss on iteration 118 = 0.14770767092704773
Training loss on iteration 119 = 0.32006922364234924
Training loss on iteration 120 = 0.21636173129081726
Training loss on iteration 121 = 0.08927173167467117
Training loss on iteration 122 = 0.24257120490074158
Training loss on iteration 123 = 0.434950053691864
Training loss on iteration 124 = 0.2599606215953827
Training loss on iteration 0 = 0.0808989554643631
Training loss on iteration 1 = 0.09953995794057846
Training loss on iteration 2 = 0.16021767258644104
Training loss on iteration 3 = 0.12216956168413162
Training loss on iteration 4 = 0.07432907074689865
Training loss on iteration 5 = 0.2796037495136261
Training loss on iteration 6 = 0.10554303228855133
Training loss on iteration 7 = 0.31536582112312317
Training loss on iteration 8 = 0.060098834335803986
Training loss on iteration 9 = 0.14068563282489777
Training loss on iteration 10 = 0.10912604629993439
Training loss on iteration 11 = 0.19428317248821259
Training loss on iteration 12 = 0.13953857123851776
Training loss on iteration 13 = 0.11972986161708832
Training loss on iteration 14 = 0.033947575837373734
Training loss on iteration 15 = 0.10127637535333633
Training loss on iteration 16 = 0.06221999228000641
Training loss on iteration 17 = 0.07015147805213928
Training loss on iteration 18 = 0.010893663391470909
Training loss on iteration 19 = 0.037840425968170166
Training loss on iteration 20 = 0.11466127634048462
Training loss on iteration 21 = 0.122455894947052
Training loss on iteration 22 = 0.1693018078804016
Training loss on iteration 23 = 0.2325991988182068
Training loss on iteration 24 = 0.11093579977750778
Training loss on iteration 25 = 0.050376471132040024
Training loss on iteration 26 = 0.23176121711730957
Training loss on iteration 27 = 0.22730107605457306
Training loss on iteration 28 = 0.1180117204785347
Training loss on iteration 29 = 0.20593908429145813
Training loss on iteration 30 = 0.16723674535751343
Training loss on iteration 31 = 0.21235981583595276
Training loss on iteration 32 = 0.048339009284973145
Training loss on iteration 33 = 0.023788684979081154
Training loss on iteration 34 = 0.0663992390036583
Training loss on iteration 35 = 0.18582405149936676
Training loss on iteration 36 = 0.06696157902479172
Training loss on iteration 37 = 0.18603917956352234
Training loss on iteration 38 = 0.15512560307979584
Training loss on iteration 39 = 0.03449035808444023
Training loss on iteration 40 = 0.12144496291875839
Training loss on iteration 41 = 0.12789423763751984
Training loss on iteration 42 = 0.3622889220714569
Training loss on iteration 43 = 0.05074624717235565
Training loss on iteration 44 = 0.07510887831449509
Training loss on iteration 45 = 0.04818195849657059
Training loss on iteration 46 = 0.2546229958534241
Training loss on iteration 47 = 0.18774846196174622
Training loss on iteration 48 = 0.10435997694730759
Training loss on iteration 49 = 0.16502058506011963
Training loss on iteration 50 = 0.07981119304895401
Training loss on iteration 51 = 0.3702786862850189
Training loss on iteration 52 = 0.19023585319519043
Training loss on iteration 53 = 0.14015071094036102
Training loss on iteration 54 = 0.09254568070173264
Training loss on iteration 55 = 0.10102624446153641
Training loss on iteration 56 = 0.09306392818689346
Training loss on iteration 57 = 0.3206806480884552
Training loss on iteration 58 = 0.07882819324731827
Training loss on iteration 59 = 0.21439191699028015
Training loss on iteration 60 = 0.14482851326465607
Training loss on iteration 61 = 0.07602435350418091
Training loss on iteration 62 = 0.21014103293418884
Training loss on iteration 63 = 0.11134637147188187
Training loss on iteration 64 = 0.04219958186149597
Training loss on iteration 65 = 0.141257643699646
Training loss on iteration 66 = 0.1720208078622818
Training loss on iteration 67 = 0.04970273748040199
Training loss on iteration 68 = 0.3179681897163391
Training loss on iteration 69 = 0.11573228240013123
Training loss on iteration 70 = 0.1838904470205307
Training loss on iteration 71 = 0.20930838584899902
Training loss on iteration 72 = 0.14300090074539185
Training loss on iteration 73 = 0.06096482649445534
Training loss on iteration 74 = 0.25041210651397705
Training loss on iteration 75 = 0.13603796064853668
Training loss on iteration 76 = 0.12671945989131927
Training loss on iteration 77 = 0.09167096018791199
Training loss on iteration 78 = 0.28519850969314575
Training loss on iteration 79 = 0.11128680408000946
Training loss on iteration 80 = 0.1190251037478447
Training loss on iteration 81 = 0.1467772275209427
Training loss on iteration 82 = 0.15764760971069336
Training loss on iteration 83 = 0.2756902873516083
Training loss on iteration 84 = 0.06395141035318375
Training loss on iteration 85 = 0.13079522550106049
Training loss on iteration 86 = 0.12306571751832962
Training loss on iteration 87 = 0.09624526649713516
Training loss on iteration 88 = 0.12900862097740173
Training loss on iteration 89 = 0.3264206349849701
Training loss on iteration 90 = 0.09345252811908722
Training loss on iteration 91 = 0.21188223361968994
Training loss on iteration 92 = 0.1192072406411171
Training loss on iteration 93 = 0.08081064373254776
Training loss on iteration 94 = 0.07144695520401001
Training loss on iteration 95 = 0.23716731369495392
Training loss on iteration 96 = 0.21175000071525574
Training loss on iteration 97 = 0.052798040211200714
Training loss on iteration 98 = 0.03730909898877144
Training loss on iteration 99 = 0.09182147681713104
Training loss on iteration 100 = 0.03486121818423271
Training loss on iteration 101 = 0.06873281300067902
Training loss on iteration 102 = 0.12174412608146667
Training loss on iteration 103 = 0.0890858992934227
Training loss on iteration 104 = 0.2170104682445526
Training loss on iteration 105 = 0.0722266435623169
Training loss on iteration 106 = 0.09384739398956299
Training loss on iteration 107 = 0.10340684652328491
Training loss on iteration 108 = 0.01480814814567566
Training loss on iteration 109 = 0.09715589880943298
Training loss on iteration 110 = 0.12677977979183197
Training loss on iteration 111 = 0.12291441112756729
Training loss on iteration 112 = 0.14549526572227478
Training loss on iteration 113 = 0.057483136653900146
Training loss on iteration 114 = 0.09106074273586273
Training loss on iteration 115 = 0.1284010261297226
Training loss on iteration 116 = 0.14087067544460297
Training loss on iteration 117 = 0.10794317722320557
Training loss on iteration 118 = 0.2655087113380432
Training loss on iteration 119 = 0.09283043444156647
Training loss on iteration 120 = 0.09161503612995148
Training loss on iteration 121 = 0.010362311266362667
Training loss on iteration 122 = 0.3293544352054596
Training loss on iteration 123 = 0.4048781991004944
Training loss on iteration 124 = 0.42033302783966064
Training loss on iteration 0 = 0.030798297375440598
Training loss on iteration 1 = 0.20845435559749603
Training loss on iteration 2 = 0.0939016342163086
Training loss on iteration 3 = 0.10499776154756546
Training loss on iteration 4 = 0.20378641784191132
Training loss on iteration 5 = 0.08520015329122543
Training loss on iteration 6 = 0.01988542079925537
Training loss on iteration 7 = 0.1687614470720291
Training loss on iteration 8 = 0.09392671287059784
Training loss on iteration 9 = 0.06692779064178467
Training loss on iteration 10 = 0.05474037304520607
Training loss on iteration 11 = 0.13197839260101318
Training loss on iteration 12 = 0.06873401999473572
Training loss on iteration 13 = 0.10057545453310013
Training loss on iteration 14 = 0.1029607355594635
Training loss on iteration 15 = 0.05352839082479477
Training loss on iteration 16 = 0.1722303330898285
Training loss on iteration 17 = 0.07273834943771362
Training loss on iteration 18 = 0.09109961986541748
Training loss on iteration 19 = 0.04670469090342522
Training loss on iteration 20 = 0.019156863912940025
Training loss on iteration 21 = 0.10119213908910751
Training loss on iteration 22 = 0.05257014185190201
Training loss on iteration 23 = 0.058961547911167145
Training loss on iteration 24 = 0.052487872540950775
Training loss on iteration 25 = 0.10387817025184631
Training loss on iteration 26 = 0.04700092971324921
Training loss on iteration 27 = 0.021539414301514626
Training loss on iteration 28 = 0.03260409086942673
Training loss on iteration 29 = 0.10957124829292297
Training loss on iteration 30 = 0.08249987661838531
Training loss on iteration 31 = 0.07810000330209732
Training loss on iteration 32 = 0.0806104838848114
Training loss on iteration 33 = 0.05687521770596504
Training loss on iteration 34 = 0.07510829716920853
Training loss on iteration 35 = 0.04270577058196068
Training loss on iteration 36 = 0.1988270878791809
Training loss on iteration 37 = 0.16178587079048157
Training loss on iteration 38 = 0.04100165143609047
Training loss on iteration 39 = 0.2257024198770523
Training loss on iteration 40 = 0.12476389110088348
Training loss on iteration 41 = 0.06231702119112015
Training loss on iteration 42 = 0.25891929864883423
Training loss on iteration 43 = 0.17022216320037842
Training loss on iteration 44 = 0.06762932240962982
Training loss on iteration 45 = 0.13957378268241882
Training loss on iteration 46 = 0.2770500183105469
Training loss on iteration 47 = 0.12130771577358246
Training loss on iteration 48 = 0.07502806186676025
Training loss on iteration 49 = 0.1451021432876587
Training loss on iteration 50 = 0.07917270809412003
Training loss on iteration 51 = 0.08397094905376434
Training loss on iteration 52 = 0.12360896170139313
Training loss on iteration 53 = 0.08884177356958389
Training loss on iteration 54 = 0.0788058191537857
Training loss on iteration 55 = 0.03916538506746292
Training loss on iteration 56 = 0.12164587527513504
Training loss on iteration 57 = 0.2462003082036972
Training loss on iteration 58 = 0.04830051213502884
Training loss on iteration 59 = 0.3095683753490448
Training loss on iteration 60 = 0.34921175241470337
Training loss on iteration 61 = 0.23568660020828247
Training loss on iteration 62 = 0.15743067860603333
Training loss on iteration 63 = 0.03624909743666649
Training loss on iteration 64 = 0.13495764136314392
Training loss on iteration 65 = 0.1290750801563263
Training loss on iteration 66 = 0.06015833094716072
Training loss on iteration 67 = 0.07311451435089111
Training loss on iteration 68 = 0.0315743014216423
Training loss on iteration 69 = 0.035766493529081345
Training loss on iteration 70 = 0.16312064230442047
Training loss on iteration 71 = 0.0766930878162384
Training loss on iteration 72 = 0.05877908319234848
Training loss on iteration 73 = 0.13634531199932098
Training loss on iteration 74 = 0.2786218523979187
Training loss on iteration 75 = 0.1491585075855255
Training loss on iteration 76 = 0.1447349637746811
Training loss on iteration 77 = 0.1695052534341812
Training loss on iteration 78 = 0.0790977030992508
Training loss on iteration 79 = 0.04037872329354286
Training loss on iteration 80 = 0.0534178726375103
Training loss on iteration 81 = 0.11163871735334396
Training loss on iteration 82 = 0.07658623903989792
Training loss on iteration 83 = 0.21600234508514404
Training loss on iteration 84 = 0.1486538201570511
Training loss on iteration 85 = 0.32557201385498047
Training loss on iteration 86 = 0.035144027322530746
Training loss on iteration 87 = 0.12202541530132294
Training loss on iteration 88 = 0.16745880246162415
Training loss on iteration 89 = 0.1935570240020752
Training loss on iteration 90 = 0.0960347130894661
Training loss on iteration 91 = 0.15839633345603943
Training loss on iteration 92 = 0.09016747772693634
Training loss on iteration 93 = 0.28857043385505676
Training loss on iteration 94 = 0.06424026191234589
Training loss on iteration 95 = 0.047265876084566116
Training loss on iteration 96 = 0.04967284947633743
Training loss on iteration 97 = 0.06844864040613174
Training loss on iteration 98 = 0.2577345371246338
Training loss on iteration 99 = 0.10117105394601822
Training loss on iteration 100 = 0.07583174854516983
Training loss on iteration 101 = 0.2432273030281067
Training loss on iteration 102 = 0.23616015911102295
Training loss on iteration 103 = 0.12181207537651062
Training loss on iteration 104 = 0.28101053833961487
Training loss on iteration 105 = 0.17052629590034485
Training loss on iteration 106 = 0.08152204751968384
Training loss on iteration 107 = 0.12927213311195374
Training loss on iteration 108 = 0.10831911116838455
Training loss on iteration 109 = 0.22242361307144165
Training loss on iteration 110 = 0.05241864547133446
Training loss on iteration 111 = 0.07902760058641434
Training loss on iteration 112 = 0.08430982381105423
Training loss on iteration 113 = 0.2641379237174988
Training loss on iteration 114 = 0.4951399564743042
Training loss on iteration 115 = 0.05384088307619095
Training loss on iteration 116 = 0.09886009991168976
Training loss on iteration 117 = 0.070299431681633
Training loss on iteration 118 = 0.1594933569431305
Training loss on iteration 119 = 0.12989439070224762
Training loss on iteration 120 = 0.33653905987739563
Training loss on iteration 121 = 0.32582151889801025
Training loss on iteration 122 = 0.0811232477426529
Training loss on iteration 123 = 0.16073495149612427
Training loss on iteration 124 = 0.19782669842243195
Training loss on iteration 0 = 0.058620840311050415
Training loss on iteration 1 = 0.35233333706855774
Training loss on iteration 2 = 0.11802146583795547
Training loss on iteration 3 = 0.11280599981546402
Training loss on iteration 4 = 0.018343841657042503
Training loss on iteration 5 = 0.11190308630466461
Training loss on iteration 6 = 0.15066245198249817
Training loss on iteration 7 = 0.10183104127645493
Training loss on iteration 8 = 0.13320370018482208
Training loss on iteration 9 = 0.04574399068951607
Training loss on iteration 10 = 0.09530508518218994
Training loss on iteration 11 = 0.10024381428956985
Training loss on iteration 12 = 0.016684237867593765
Training loss on iteration 13 = 0.03334847092628479
Training loss on iteration 14 = 0.11465896666049957
Training loss on iteration 15 = 0.04092446714639664
Training loss on iteration 16 = 0.18792080879211426
Training loss on iteration 17 = 0.005259824451059103
Training loss on iteration 18 = 0.04527486860752106
Training loss on iteration 19 = 0.3701944053173065
Training loss on iteration 20 = 0.17454300820827484
Training loss on iteration 21 = 0.06719870865345001
Training loss on iteration 22 = 0.027276095002889633
Training loss on iteration 23 = 0.04019789770245552
Training loss on iteration 24 = 0.02562600001692772
Training loss on iteration 25 = 0.0668577179312706
Training loss on iteration 26 = 0.08495734632015228
Training loss on iteration 27 = 0.04709240421652794
Training loss on iteration 28 = 0.12514707446098328
Training loss on iteration 29 = 0.05653618276119232
Training loss on iteration 30 = 0.06661675870418549
Training loss on iteration 31 = 0.24842214584350586
Training loss on iteration 32 = 0.02725115232169628
Training loss on iteration 33 = 0.07690946012735367
Training loss on iteration 34 = 0.08907546103000641
Training loss on iteration 35 = 0.079110287129879
Training loss on iteration 36 = 0.33201777935028076
Training loss on iteration 37 = 0.03724706545472145
Training loss on iteration 38 = 0.13936084508895874
Training loss on iteration 39 = 0.0817067101597786
Training loss on iteration 40 = 0.1935890167951584
Training loss on iteration 41 = 0.04341180622577667
Training loss on iteration 42 = 0.09612924605607986
Training loss on iteration 43 = 0.09605161845684052
Training loss on iteration 44 = 0.04750360921025276
Training loss on iteration 45 = 0.03319183737039566
Training loss on iteration 46 = 0.1766163408756256
Training loss on iteration 47 = 0.04332197457551956
Training loss on iteration 48 = 0.058952800929546356
Training loss on iteration 49 = 0.08963291347026825
Training loss on iteration 50 = 0.024094559252262115
Training loss on iteration 51 = 0.15096484124660492
Training loss on iteration 52 = 0.10986270755529404
Training loss on iteration 53 = 0.03311750665307045
Training loss on iteration 54 = 0.05291285365819931
Training loss on iteration 55 = 0.06882841140031815
Training loss on iteration 56 = 0.0357261523604393
Training loss on iteration 57 = 0.018652327358722687
Training loss on iteration 58 = 0.1473514884710312
Training loss on iteration 59 = 0.054353244602680206
Training loss on iteration 60 = 0.09313429147005081
Training loss on iteration 61 = 0.06895507127046585
Training loss on iteration 62 = 0.10293440520763397
Training loss on iteration 63 = 0.07130642235279083
Training loss on iteration 64 = 0.03089331090450287
Training loss on iteration 65 = 0.09397917240858078
Training loss on iteration 66 = 0.12996895611286163
Training loss on iteration 67 = 0.0534399077296257
Training loss on iteration 68 = 0.021709667518734932
Training loss on iteration 69 = 0.04733244702219963
Training loss on iteration 70 = 0.051806844770908356
Training loss on iteration 71 = 0.012696261517703533
Training loss on iteration 72 = 0.05927698686718941
Training loss on iteration 73 = 0.214998260140419
Training loss on iteration 74 = 0.030776308849453926
Training loss on iteration 75 = 0.18639887869358063
Training loss on iteration 76 = 0.1065344363451004
Training loss on iteration 77 = 0.012836216017603874
Training loss on iteration 78 = 0.06816529482603073
Training loss on iteration 79 = 0.10914962738752365
Training loss on iteration 80 = 0.12677736580371857
Training loss on iteration 81 = 0.1880294531583786
Training loss on iteration 82 = 0.07552632689476013
Training loss on iteration 83 = 0.16306160390377045
Training loss on iteration 84 = 0.06964641064405441
Training loss on iteration 85 = 0.17307935655117035
Training loss on iteration 86 = 0.16816791892051697
Training loss on iteration 87 = 0.017741575837135315
Training loss on iteration 88 = 0.03625616431236267
Training loss on iteration 89 = 0.038165464997291565
Training loss on iteration 90 = 0.20481061935424805
Training loss on iteration 91 = 0.15091681480407715
Training loss on iteration 92 = 0.061907947063446045
Training loss on iteration 93 = 0.2791902422904968
Training loss on iteration 94 = 0.10684365034103394
Training loss on iteration 95 = 0.22242723405361176
Training loss on iteration 96 = 0.07260533422231674
Training loss on iteration 97 = 0.28661665320396423
Training loss on iteration 98 = 0.31079936027526855
Training loss on iteration 99 = 0.34262850880622864
Training loss on iteration 100 = 0.058360885828733444
Training loss on iteration 101 = 0.25187861919403076
Training loss on iteration 102 = 0.12141840159893036
Training loss on iteration 103 = 0.16667227447032928
Training loss on iteration 104 = 0.11189917474985123
Training loss on iteration 105 = 0.028493966907262802
Training loss on iteration 106 = 0.12390829622745514
Training loss on iteration 107 = 0.048952776938676834
Training loss on iteration 108 = 0.31799307465553284
Training loss on iteration 109 = 0.14985091984272003
Training loss on iteration 110 = 0.1400429606437683
Training loss on iteration 111 = 0.10905139148235321
Training loss on iteration 112 = 0.12359330803155899
Training loss on iteration 113 = 0.0946001186966896
Training loss on iteration 114 = 0.04809875041246414
Training loss on iteration 115 = 0.045751482248306274
Training loss on iteration 116 = 0.09527157992124557
Training loss on iteration 117 = 0.09988774359226227
Training loss on iteration 118 = 0.2286340594291687
Training loss on iteration 119 = 0.09335200488567352
Training loss on iteration 120 = 0.14580412209033966
Training loss on iteration 121 = 0.02841617725789547
Training loss on iteration 122 = 0.03973773121833801
Training loss on iteration 123 = 0.0358714796602726
Training loss on iteration 124 = 0.06711395829916
Training loss on iteration 0 = 0.07546144723892212
Training loss on iteration 1 = 0.20092889666557312
Training loss on iteration 2 = 0.026525460183620453
Training loss on iteration 3 = 0.013071243651211262
Training loss on iteration 4 = 0.003516881261020899
Training loss on iteration 5 = 0.0340132862329483
Training loss on iteration 6 = 0.0173274427652359
Training loss on iteration 7 = 0.25067687034606934
Training loss on iteration 8 = 0.048291854560375214
Training loss on iteration 9 = 0.021677156910300255
Training loss on iteration 10 = 0.022304022684693336
Training loss on iteration 11 = 0.08655914664268494
Training loss on iteration 12 = 0.24754513800144196
Training loss on iteration 13 = 0.0919627696275711
Training loss on iteration 14 = 0.1715860813856125
Training loss on iteration 15 = 0.02683372236788273
Training loss on iteration 16 = 0.11980637907981873
Training loss on iteration 17 = 0.11867140233516693
Training loss on iteration 18 = 0.055524542927742004
Training loss on iteration 19 = 0.17192286252975464
Training loss on iteration 20 = 0.053778667002916336
Training loss on iteration 21 = 0.14262749254703522
Training loss on iteration 22 = 0.06187799572944641
Training loss on iteration 23 = 0.030200131237506866
Training loss on iteration 24 = 0.05506862699985504
Training loss on iteration 25 = 0.06497563421726227
Training loss on iteration 26 = 0.022150356322526932
Training loss on iteration 27 = 0.025171929970383644
Training loss on iteration 28 = 0.0535963736474514
Training loss on iteration 29 = 0.09939341992139816
Training loss on iteration 30 = 0.057793859392404556
Training loss on iteration 31 = 0.22359928488731384
Training loss on iteration 32 = 0.09778022766113281
Training loss on iteration 33 = 0.036096759140491486
Training loss on iteration 34 = 0.029158838093280792
Training loss on iteration 35 = 0.02687247283756733
Training loss on iteration 36 = 0.08145170658826828
Training loss on iteration 37 = 0.19557860493659973
Training loss on iteration 38 = 0.009360970929265022
Training loss on iteration 39 = 0.20307093858718872
Training loss on iteration 40 = 0.04604174196720123
Training loss on iteration 41 = 0.17449401319026947
Training loss on iteration 42 = 0.18192771077156067
Training loss on iteration 43 = 0.010884314775466919
Training loss on iteration 44 = 0.34243807196617126
Training loss on iteration 45 = 0.058111611753702164
Training loss on iteration 46 = 0.12932191789150238
Training loss on iteration 47 = 0.09404373168945312
Training loss on iteration 48 = 0.05778113007545471
Training loss on iteration 49 = 0.06948674470186234
Training loss on iteration 50 = 0.05539104714989662
Training loss on iteration 51 = 0.1908922791481018
Training loss on iteration 52 = 0.19050216674804688
Training loss on iteration 53 = 0.4182420074939728
Training loss on iteration 54 = 0.02961418218910694
Training loss on iteration 55 = 0.09292421489953995
Training loss on iteration 56 = 0.10948257148265839
Training loss on iteration 57 = 0.07925698906183243
Training loss on iteration 58 = 0.03294267877936363
Training loss on iteration 59 = 0.14050741493701935
Training loss on iteration 60 = 0.28346580266952515
Training loss on iteration 61 = 0.0985640287399292
Training loss on iteration 62 = 0.03819684311747551
Training loss on iteration 63 = 0.06382983177900314
Training loss on iteration 64 = 0.06808795034885406
Training loss on iteration 65 = 0.24287211894989014
Training loss on iteration 66 = 0.1279284954071045
Training loss on iteration 67 = 0.10930214822292328
Training loss on iteration 68 = 0.053691565990448
Training loss on iteration 69 = 0.194729745388031
Training loss on iteration 70 = 0.14682838320732117
Training loss on iteration 71 = 0.1559913456439972
Training loss on iteration 72 = 0.09761841595172882
Training loss on iteration 73 = 0.09262344241142273
Training loss on iteration 74 = 0.15029951930046082
Training loss on iteration 75 = 0.12402554601430893
Training loss on iteration 76 = 0.09186519682407379
Training loss on iteration 77 = 0.15602320432662964
Training loss on iteration 78 = 0.03252936527132988
Training loss on iteration 79 = 0.05297532305121422
Training loss on iteration 80 = 0.013340383768081665
Training loss on iteration 81 = 0.18189853429794312
Training loss on iteration 82 = 0.04291745647788048
Training loss on iteration 83 = 0.10324813425540924
Training loss on iteration 84 = 0.0972914770245552
Training loss on iteration 85 = 0.046406831592321396
Training loss on iteration 86 = 0.1352320909500122
Training loss on iteration 87 = 0.2332359105348587
Training loss on iteration 88 = 0.09300419688224792
Training loss on iteration 89 = 0.07682549208402634
Training loss on iteration 90 = 0.02535230480134487
Training loss on iteration 91 = 0.09967080503702164
Training loss on iteration 92 = 0.03891751915216446
Training loss on iteration 93 = 0.0907798558473587
Training loss on iteration 94 = 0.08340650796890259
Training loss on iteration 95 = 0.1825493723154068
Training loss on iteration 96 = 0.23757268488407135
Training loss on iteration 97 = 0.03355615213513374
Training loss on iteration 98 = 0.08496824651956558
Training loss on iteration 99 = 0.14823833107948303
Training loss on iteration 100 = 0.16715142130851746
Training loss on iteration 101 = 0.1299339234828949
Training loss on iteration 102 = 0.07798381894826889
Training loss on iteration 103 = 0.04012387618422508
Training loss on iteration 104 = 0.14667344093322754
Training loss on iteration 105 = 0.023218797519803047
Training loss on iteration 106 = 0.09094323217868805
Training loss on iteration 107 = 0.04854960739612579
Training loss on iteration 108 = 0.18896670639514923
Training loss on iteration 109 = 0.05145517736673355
Training loss on iteration 110 = 0.0353882871568203
Training loss on iteration 111 = 0.1457904428243637
Training loss on iteration 112 = 0.08922729641199112
Training loss on iteration 113 = 0.16423460841178894
Training loss on iteration 114 = 0.07923437654972076
Training loss on iteration 115 = 0.1783176064491272
Training loss on iteration 116 = 0.0764528438448906
Training loss on iteration 117 = 0.0411502979695797
Training loss on iteration 118 = 0.11703570932149887
Training loss on iteration 119 = 0.08093366026878357
Training loss on iteration 120 = 0.013879532925784588
Training loss on iteration 121 = 0.06780724227428436
Training loss on iteration 122 = 0.04536641016602516
Training loss on iteration 123 = 0.19124279916286469
Training loss on iteration 124 = 0.17304778099060059
